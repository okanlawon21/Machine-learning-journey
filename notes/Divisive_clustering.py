import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs
from scipy.cluster.hierarchy import dendrogram, linkage


X, _ = make_blobs(n_samples=30, centers=5, cluster_std=10, random_state=42)

def divisive_clustering(data, max_clusters=3):
    clusters = [data]
    while len(clusters) < max_clusters:

        cluster_to_split = max(clusters, key=lambda x: len(x))
        index = clusters.index(cluster_to_split)
        clusters.pop(index)

        kmeans = KMeans(n_clusters=2, random_state=42).fit(cluster_to_split)
        cluster1 = cluster_to_split[kmeans.labels_ == 0]
        cluster2 = cluster_to_split[kmeans.labels_ == 1]


        if cluster1.shape[0] > 0:
            clusters.append(cluster1)
        if cluster2.shape[0] > 0:
            clusters.append(cluster2)
    return clusters

clusters = divisive_clustering(X, max_clusters=3)

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
colors = ['r', 'g', 'b', 'c', 'm', 'y']
for i, cluster in enumerate(clusters):
    if isinstance(cluster, np.ndarray) and cluster.shape[0] > 0:
        plt.scatter(cluster[:, 0], cluster[:, 1], s=50,
                    c=colors[i % len(colors)], label=f'Cluster {i+1}')
plt.title('Divisive Clustering Result')
plt.legend()

linked = linkage(X, method='ward')

plt.subplot(1, 2, 2)
dendrogram(linked, orientation='top',
           distance_sort='descending', show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')

plt.tight_layout()
plt.show()